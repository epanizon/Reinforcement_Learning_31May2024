{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d9c012e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.autograd \n",
    "import torch.optim as optim \n",
    "\n",
    "import numpy as np \n",
    "import gym \n",
    "from collections import deque \n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55b23f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module): \n",
    "    \n",
    "    def __init__(self, input_dim, fc1_dim, fc2_dim, n_actions, name=None, chkpt=\"model\"): \n",
    "        \n",
    "        super(Critic, self).__init__() \n",
    "        \n",
    "        self.input_dim = input_dim \n",
    "        self.fc1_dim = fc1_dim \n",
    "        self.fc2_dim = fc2_dim \n",
    "        self.n_actions = n_actions \n",
    "        \n",
    "        # name of model to save\n",
    "        if name is not None:\n",
    "            if not os.path.exists(chkpt): \n",
    "                os.makedirs(chkpt)\n",
    "            self.filename = os.path.join(chkpt, name +'_ddpg')\n",
    "        \n",
    "        self.fc1 = nn.Linear(*self.input_dim, self.fc1_dim)\n",
    "        self.bn1 = nn.LayerNorm(self.fc1_dim)\n",
    "        self.fc2 = nn.Linear(self.fc1_dim, self.fc2_dim)\n",
    "        self.bn2 = nn.LayerNorm(self.fc2_dim)\n",
    "        self.action_value = nn.Linear(self.n_actions,fc2_dim)\n",
    "        self.q = nn.Linear(self.fc2_dim,1)\n",
    "        \n",
    "    def forward(self, state, action): \n",
    "        \n",
    "        state_value = self.fc1(state)\n",
    "        state_value = self.bn1(state_value)\n",
    "        state_value = F.relu(state_value)\n",
    "        state_value = self.fc2(state_value) \n",
    "        state_value = self.bn2(state_value) \n",
    "        action_value = F.relu(self.action_value(action))\n",
    "        state_action_value = F.relu(torch.add(state_value,action_value))  \n",
    "        state_action_value = self.q(state_action_value)\n",
    "        return state_action_value \n",
    "    \n",
    "    def init_weights(self): \n",
    "        \n",
    "        f1 = 1 / np.sqrt(self.fc1.weight.data.size()[0])\n",
    "        f2 = 1 / np.sqrt(self.fc2.weight.data.size()[0])\n",
    "        f3 = 0.003\n",
    "        \n",
    "        torch.nn.init.uniform_(self.fc1.weight.data, -f1, f1)\n",
    "        torch.nn.init.uniform_(self.fc1.bias.data, -f1, f1)\n",
    "        torch.nn.init.uniform_(self.fc2.weight.data, -f2, f2)\n",
    "        torch.nn.init.uniform_(self.fc2.bias.data, -f2, f2)\n",
    "        torch.nn.init.uniform_(self.q.weight.data, -f3, f3)\n",
    "        torch.nn.init.uniform_(self.q.bias.data, -f3, f3)\n",
    "        \n",
    "        \n",
    "    def save_checkpoint(self):\n",
    "        torch.save(self.state_dict(), self.filename)\n",
    "        print(\"saving\")\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(torch.load(self.filename))\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c627e8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "momo = Critic([3], 4, 5, 2, name=None, chkpt=\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8a3db9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class newCritic(nn.Module): \n",
    "    \n",
    "    def __init__(self, input_dim, hidden_layers_dims, n_actions, name=None, chkpt=\"model\"): \n",
    "        \n",
    "        super(newCritic, self).__init__() \n",
    "        \n",
    "        # hidden_layers_dims is a list with dimension of all hidden layers.\n",
    "        # number of hidden_layers is inferred by its length\n",
    "        self.hidden_layers_dims = input_dim + hidden_layers_dims\n",
    "        self.n_actions = n_actions \n",
    "        \n",
    "        # name of model to save\n",
    "        if name is not None:\n",
    "            if not os.path.exists(chkpt): \n",
    "                os.makedirs(chkpt)\n",
    "            self.filename = os.path.join(chkpt, name +'_ddpg')\n",
    "\n",
    "        # hidden_layers are linear + layernorm. \n",
    "        self.hidden_layers = []\n",
    "        for dim_in, dim_out in zip(self.hidden_layers_dims[:-1],self.hidden_layers_dims[1:]): \n",
    "            self.hidden_layers.append( nn.Linear( dim_in, dim_out) )\n",
    "            self.hidden_layers.append( nn.LayerNorm(dim_out) )\n",
    "            self.hidden_layers.append( F.relu )\n",
    "\n",
    "        last_hidden_layer_dim = self.hidden_layers_dims[-1]\n",
    "\n",
    "        # a linear layer is constructed from the actions directly to the last hidden layer \n",
    "        self.action_value = nn.Linear(self.n_actions, last_hidden_layer_dim)\n",
    "                                      \n",
    "        # this is the final layer which returns the quality function q(s,a)\n",
    "        self.q = nn.Linear(last_hidden_layer_dim,1)\n",
    "        \n",
    "    def forward(self, state, action): \n",
    "        \n",
    "        # first \"branch\" from input state to final hidden layer\n",
    "        state_value = state\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            state_value = hidden_layer(state_value)\n",
    "        \n",
    "        # second \"branch\" from action to final hidden layer\n",
    "        action_value = F.relu(self.action_value(action))\n",
    "\n",
    "        # merge of the two branches\n",
    "        state_action_value = F.relu(torch.add(state_value,action_value))\n",
    "        \n",
    "        # evaluation of q(s,a)\n",
    "        state_action_value = self.q(state_action_value)\n",
    "        return state_action_value \n",
    "    \n",
    "    # q is initialized to smaller values than what \"suggested\" by the 1/sqrt(input_dim) rule\n",
    "    def init_weights(self): \n",
    "        \n",
    "        init_weights_q = 0.003\n",
    "        torch.nn.init.uniform_(self.q.weight.data, -init_weights_q, init_weights_q)\n",
    "        torch.nn.init.uniform_(self.q.bias.data,   -init_weights_q, init_weights_q)\n",
    "        \n",
    "    # saves checkpoint for the model\n",
    "    def save_checkpoint(self):\n",
    "        torch.save(self.state_dict(), self.filename)\n",
    "        print(\"saving\")\n",
    "\n",
    "    # loads checkpoint for the model\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(torch.load(self.filename))\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fd231358",
   "metadata": {},
   "outputs": [],
   "source": [
    "newmomo = newCritic([3], [4, 5,10,2,4], 2, name=None, chkpt=\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "13445355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "momo.fc1.weight.data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "027988e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype, a.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ad775990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0172], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([0.,-0.2,0.3])\n",
    "a = torch.tensor([1.,0.])\n",
    "momo(x,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "44aa85e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1539], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newmomo(x,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW ACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_layers_dims, n_actions, name=None, chkpt=\"model\"): \n",
    "        \n",
    "        super(Actor, self).__init__() \n",
    "            \n",
    "        # hidden_layers_dims is a list with dimension of all hidden layers.\n",
    "        # number of hidden_layers is inferred by its length\n",
    "        self.hidden_layers_dims = input_dim + hidden_layers_dims\n",
    "        self.n_actions = n_actions \n",
    "        \n",
    "        # name of model to save\n",
    "        if name is not None:\n",
    "            if not os.path.exists(chkpt): \n",
    "                os.makedirs(chkpt)\n",
    "            self.filename = os.path.join(chkpt, name +'_ddpg')\n",
    "\n",
    "        # hidden_layers are linear + layernorm. \n",
    "        self.hidden_layers_list = []\n",
    "        for dim_in, dim_out in zip(self.hidden_layers_dims[:-1],self.hidden_layers_dims[1:]): \n",
    "            self.hidden_layers_list.append( nn.Linear( dim_in, dim_out) )\n",
    "            self.hidden_layers_list.append( nn.LayerNorm(dim_out) )\n",
    "            self.hidden_layers_list.append( nn.ReLU() )\n",
    "\n",
    "        last_hidden_layer_dim = self.hidden_layers_dims[-1]\n",
    "        # a linear layer is constructed from the last hidden layer to the action  \n",
    "        self.hidden_layers_list.append( nn.Linear(last_hidden_layer_dim, self.n_actions) )\n",
    "        \n",
    "        # creates a torch Module, to make parameters visible to torch optimizer\n",
    "        self.hidden_layers = nn.Sequential(*self.hidden_layers_list)\n",
    "\n",
    "    def forward(self,state):\n",
    "        \n",
    "        # from input state to final hidden layer to squashed mu\n",
    "        x = torch.tanh( self.hidden_layers( state) )\n",
    "        return x\n",
    "    \n",
    "    # q is initialized to smaller values than what \"suggested\" by the 1/sqrt(input_dim) rule\n",
    "    def init_weights(self): \n",
    "        \n",
    "        init_weights_mu = 0.003\n",
    "        torch.nn.init.uniform_(self.hidden_layers_dims[-1].weight.data, -init_weights_mu, init_weights_mu)\n",
    "        torch.nn.init.uniform_(self.hidden_layers_dims[-1].bias.data,   -init_weights_mu, init_weights_mu)\n",
    "        \n",
    "        \n",
    "    # saves checkpoint for the model\n",
    "    def save_checkpoint(self):\n",
    "        torch.save(self.state_dict(), self.filename)\n",
    "        print(\"saving\")\n",
    "\n",
    "    # loads checkpoint for the model\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(torch.load(self.filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "momo = Actor([3], [4, 5,10,2,4], 2, name=None, chkpt=\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5513, -0.5565], grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([0.,-0.2,0.3])\n",
    "momo(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New DDPGAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGagent: \n",
    "    \"\"\"\n",
    "    Initializes an agent which uses the Deep Deterministic Policy Gradient agent \n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 env, \n",
    "                 hidden_layers_dims,\n",
    "                 replay_min=100,\n",
    "                 replay_size=1000000,\n",
    "                 critic_lr=0.00015, \n",
    "                 actor_lr=0.000015, \n",
    "                 tau =0.001, \n",
    "                 gamma=0.99,\n",
    "                 loss=nn.MSELoss(), \n",
    "                 batch_size=64, \n",
    "                 name_critic=None, \n",
    "                 name_actor=None, \n",
    "                 device = \"cpu\",\n",
    "                 directory = \"models\"):\n",
    "        \n",
    "        # \"reads\" the environment and sets\n",
    "        self.env = env \n",
    "        self.input_dim = env.observation_space.shape \n",
    "        self.n_actions = env.action_space.shape[0]\n",
    "        self.tau = tau \n",
    "        self.device = device\n",
    "        self.gamma = gamma \n",
    "        \n",
    "        # \n",
    "        self.batch_size = batch_size \n",
    "        self.memory= Replay_Memory(replay_size)\n",
    "        self.replay_min = replay_min\n",
    "        \n",
    "        # sets the names used for folder creation and checkpoints saves\n",
    "        self.name_critic = name_critic\n",
    "        self.name_actor = name_actor \n",
    "        \n",
    "        # creates critic\n",
    "        self.critic = Critic(self.input_dim, hidden_layers_dims, self.n_actions, name=name_critic, chkpt=directory).to(device)\n",
    "        name_target_critic = None\n",
    "        \n",
    "        if name_critic is not None: \n",
    "            name_target_critic = name_critic + \"_target\"\n",
    "        \n",
    "        # creates target critic\n",
    "        self.target_critic = Critic(self.input_dim, hidden_layers_dims, self.n_actions, self.n_actions, name = name_target_critic,chkpt=directory).to(device)\n",
    "        \n",
    "        \n",
    "        # creates actor\n",
    "        self.actor = Actor(self.input_dim, layer1_size, layer2_size, self.n_actions, name = name_actor,chkpt=directory).to(device)\n",
    "        \n",
    "        name_target_actor = None \n",
    "        if name_actor is not None: \n",
    "                name_target_actor = name_actor + \"_target\"\n",
    "            \n",
    "        # creates target actor\n",
    "        self.target_actor = Actor(self.input_dim, layer1_size, layer2_size, self.n_actions, name = name_target_actor,chkpt=directory).to(device)\n",
    "        \n",
    "        # initialization of weights (possibly redundant)\n",
    "        self.critic.init_weights()\n",
    "        self.actor.init_weights()\n",
    "\n",
    "        # initialization of weights (possibly redundant)\n",
    "        self.update_target_weights()\n",
    "        \n",
    "        self.critic_criterion = loss \n",
    "        self.actor_criterion = loss  \n",
    "        \n",
    "        # initialization of optimizers\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),lr=critic_lr,weight_decay=0.01)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),lr=actor_lr)\n",
    "    \n",
    "    # explict decay of learning rates\n",
    "    def update_critic_optimizer(self, learning_rate):\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),lr=learning_rate)\n",
    "        \n",
    "    def update_actor_optimizer(self, learning_rate): \n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),lr=learning_rate)\n",
    "        \n",
    "    \n",
    "    def update_replay_memory(self,state, action, reward, next_state, done): \n",
    "        \"\"\" Adds single experience to the memory buffer.\n",
    "        Receives s,a,r,s',done\n",
    "        \"\"\"\n",
    "        self.memory.push(state, action, reward, next_state, done)    \n",
    "\n",
    "        \n",
    "    def update_target_weights(self,tau=1): \n",
    "        \"\"\" Soft-update of the target networks towards the current (learned) network.\n",
    "        \"\"\"\n",
    "        \n",
    "        for target_param, param in zip(self.target_critic.parameters(),self.critic.parameters()): \n",
    "            target_param.data.copy_(param.data * self.tau + target_param.data *(1.0 - tau))\n",
    "            \n",
    "        for target_param, param in zip(self.target_actor.parameters(),self.actor.parameters()): \n",
    "            target_param.data.copy_(param.data * self.tau + target_param.data *(1.0 - tau))\n",
    "            \n",
    "            \n",
    "    def get_action(self, observation): \n",
    "        \"\"\" From state (observation) to the deterministic (+noise) action\n",
    "        \"\"\"\n",
    "        self.actor.eval()  #because I have batch norm \n",
    "        \n",
    "        observation = torch.tensor(observation, dtype= torch.float).to(self.device)\n",
    "        actor_action = self.actor(observation)\n",
    "        action = actor_action.cpu().detach().numpy()  \n",
    "        \n",
    "        return action \n",
    "    \n",
    "    \n",
    "    def train(self): \n",
    "        \n",
    "        # training starts only after some sampling has been done\n",
    "        if len(self.memory) <  self.replay_min:\n",
    "            return \n",
    "        \n",
    "        # randomly sampled experience from past.\n",
    "        states, actions, rewards, next_states, not_done = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        states = torch.tensor(np.array(states), dtype = torch.float).to(self.device)\n",
    "        actions = torch.tensor(np.array(actions), dtype = torch.float).to(self.device)\n",
    "        rewards = torch.tensor(np.array(rewards), dtype = torch.float).to(self.device)\n",
    "        next_states = torch.tensor(np.array(next_states), dtype = torch.float).to(self.device)\n",
    "        not_done = torch.tensor(np.array(not_done)).unsqueeze(1).to(self.device)\n",
    "        \n",
    "        # \n",
    "        self.actor.eval()     \n",
    "        self.critic.eval() \n",
    "        \n",
    "\n",
    "        self.target_actor.eval() \n",
    "        self.target_critic.eval() \n",
    "        \n",
    "        # a' = mu(s')\n",
    "        # target q -> q(s', mu)\n",
    "        target_actions = self.target_actor.forward(next_states)\n",
    "        target_critic_value = self.target_critic(next_states, target_actions) \n",
    "    \n",
    "        # Q^exp = r + gamma Q(s',a')\n",
    "        targets = rewards + self.gamma*not_done*target_critic_value\n",
    "        targets.to(self.device)          \n",
    "        \n",
    "        self.critic.train()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_value = self.critic.forward(states, actions)\n",
    "        \n",
    "        # if MSE, loss = (r + gamma Q(s', a') - Q(s,a))^2\n",
    "        loss = self.critic_criterion(critic_value, targets)\n",
    "        loss.backward() \n",
    "        \n",
    "        self.critic_optimizer.step() \n",
    "        self.critic.eval() \n",
    "        \n",
    "        self.actor_optimizer.zero_grad() \n",
    "        self.actor.train() \n",
    "        \n",
    "        mu = self.actor.forward(states)\n",
    "        \n",
    "        # if MSE, loss =  - Q(s, mu(s))^2 \n",
    "        #     --> grad ~ -Q(s,mu) grad mu(s)\n",
    "        actor_loss = -self.critic.forward(states,mu)\n",
    "        actor_loss = torch.mean(actor_loss)\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        self.update_target_weights(self.tau)\n",
    "        \n",
    "    def save_model(self):\n",
    "        \"\"\" Saves all models' checkpoints in folder given by names\n",
    "        \"\"\"\n",
    "        self.actor.save_checkpoint()\n",
    "        self.critic.save_checkpoint()\n",
    "        self.target_actor.save_checkpoint()\n",
    "        self.target_critic.save_checkpoint()\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\" Loads all models' checkpoints in folder given by names\n",
    "        \"\"\"\n",
    "        self.actor.load_checkpoint()\n",
    "        self.critic.load_checkpoint()\n",
    "        self.target_actor.load_checkpoint()\n",
    "        self.target_critic.load_checkpoint()\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../../wavelength_cards.txt\", \"r\")\n",
    "file_data = f.read()\n",
    "list_cards = file_data.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrandom\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist_cards\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "@bot.message_handler(commands=['carta'])\n",
    "def send_welcome(message):\n",
    "    index_card = random.randint(len(list_cards))\n",
    "    card = list_cards.pop(index_card)\n",
    "    bot.reply_to(message, \"Eccoti: \\n\"+card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = (3,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module): \n",
    "        \n",
    "    def __init__(self, input_dims, hidden_layers_dims, n_actions, name= None, chkpt_dir = \"save_m_2\"): \n",
    "        \n",
    "        super(CriticNetwork, self).__init__ ()\n",
    "        \n",
    "        self.name = name\n",
    "        # name of model to save\n",
    "        if name is not None:\n",
    "            if not os.path.exists(chkpt): \n",
    "                os.makedirs(chkpt)\n",
    "            self.checkpoint_file= os.path.join(chkpt_dir,name +'_td3) \n",
    "            \n",
    "        \n",
    "        # number of hidden_layers is inferred by its length\n",
    "        self.hidden_layers_dims = list(input_dim) + hidden_layers_dims\n",
    "        self.n_actions = n_actions \n",
    "        \n",
    "        self.name = name \n",
    "        \n",
    "        #self.checkpoint_dir = chkpt_dir \n",
    "        \n",
    "        #self.checkpoint_file = os.path.join(self.checkpoint_dir, name+'_td3') \n",
    "        \n",
    "        self.fc1 = nn.Linear(self.input_dims[0]+n_actions, self.fc1_dims) \n",
    "        \n",
    "        self.fc2 = nn.Linear(self.fc1_dims+n_actions, self.fc2_dims) \n",
    "        \n",
    "        self.q1 = nn.Linear(self.fc2_dims,1) #scalar value of the critic (state-action value) \n",
    "        \n",
    "        \n",
    "        \n",
    "      \n",
    "        \n",
    "    def forward(self, state, action): \n",
    "    \n",
    "        q1_action_value = self.fc1(T.cat([state,action],dim=1))\n",
    "        \n",
    "        q1_action_value = F.relu(q1_action_value) \n",
    "        \n",
    "        #q1_action_value = self.fc2(q1_action_value)\n",
    "        \n",
    "        q1_action_value = self.fc2(T.cat([q1_action_value,action],dim=1))\n",
    "        \n",
    "        q1_action_value = F.relu(q1_action_value) \n",
    "        \n",
    "        q1 = self.q1(q1_action_value) \n",
    "        \n",
    "        return q1 \n",
    "        \n",
    "    def save_checkpoint(self): \n",
    "        \n",
    "        if self.name is not None:\n",
    "    \n",
    "            print(\"...saving...\") \n",
    "        \n",
    "            T.save(self.state_dict(),self.checkpoint_file)\n",
    "        \n",
    "        \n",
    "    def load_checkpoint(self): \n",
    "    \n",
    "        if self.name is not None:\n",
    "    \n",
    "            print(\"..loading...\") \n",
    "        \n",
    "            self.load_state_dict(T.load(self.checkpoint_file)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
